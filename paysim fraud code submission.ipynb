{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13da723c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## PAYSIM FRAUD DATASET ######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38107a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Overview:\n",
    "\n",
    "This project focuses on fraud detection using three machine learning models: Random Forest, XGBoost,\n",
    "and Multi-layer Perceptron (MLP). To ensure fair comparison, a consistent data processing and evaluation\n",
    "pipeline is implemented. The dataset's severe class imbalance is addressed using two sampling techniques:\n",
    "SMOTE and Random Under-Samplin -- enhancing minority class representation.\n",
    "\n",
    "Feature Engineering: (will be detailed in final report)\n",
    "- The \"Time\" feature is removed due to lack of informational value.\n",
    "- Features V22 to V28 are dropped because of low variance.\n",
    "\n",
    "\n",
    "***Dataset Downsampling: To address computational and runtime limitations, a stratified sampling approach was\n",
    "applied for training in some cases. This ensured the class balance was retained while significantly reducing\n",
    "the dataset size, enabling faster processing. The original dataset's large size caused system overheating\n",
    "on my local machine, making it impractical to train models without downsampling. Attempts to use Colab lead\n",
    "to challenges such as timeouts and resource issues.\n",
    "\n",
    "This approach allowed for efficient experimentation and testing, I fully acknowledged that this was far from ideal.\n",
    "Any findings or conclusions must be interpreted with this big limitation in mind.\n",
    "\n",
    "\n",
    "Avoiding Data Leakage:\n",
    "- Test data is kept separate before any transformations or sampling to ensure unbiased eval.\n",
    "- Pipeline Construction: All transformations (e.g., scaling, sampling) are applied only to training data during\n",
    "cross-val and model training, avoiding test set contamination.\n",
    "\n",
    "Model Pipeline:\n",
    "1. Scaling: StandardScaler is used.\n",
    "2. Sampling: Two techniques are tested:\n",
    "   - SMOTE: Synthesizes new instances for the minority class.\n",
    "   - Random Under-Sampling: Reduces the majority class size to improve balance.\n",
    "\n",
    "Parameter Tuning:\n",
    "- GridSearchCV optimizes hyperparams by testing combinations and selecting the best configu based on ROC AUC. \n",
    "- final model is retrained on full training set using the best hyperparams for test evaluation.\n",
    "- Metrics store in `cv_results_` allow for detailed  analysis.\n",
    "\n",
    "Cross-Val:\n",
    "- Stratified K-Fold CV (3-5 folds) ensures consistent class distribution across folds.\n",
    "- Primary metric: ROC AUC, which evaluate the model's ability to distinguish between classes.\n",
    "\n",
    "Evaluation Metrics:\n",
    "- Precision, recall, F1-score, and ROC AUC are used to assess performance. \n",
    "- Training and inference times are recorded to analyze computational efficiency.\n",
    "- Stability is measured using the SD of ROC AUC across folds.\n",
    "\n",
    "***************************************************************************************************************\n",
    "****NOTE on Thresholding: XG BOOST AND MLP ONLY ******  \n",
    "\n",
    "After selecting the best model from GridSearchCV, threshold tuning is performed on validation set.\n",
    "The default threshold of 0.5 gave poor results(very low precision), which is typical with imbalanced datasets.\n",
    "Predicted probabilities from the validation set are used to evaluate multiple thresholds, and the one \n",
    "that gave a better balance of precision/recall is chosen. \n",
    "\n",
    "Data Splitting:\n",
    "- Dataset is divided into training (60%), validation (20%), and test (20%) sets:\n",
    "- Training set: Model building and hyperparameter tuning.\n",
    "- Validation set: Threshold tuning\n",
    "- Test Set: unbiased performance evaluation.\n",
    "\n",
    "\n",
    "**************************************************************************************************************\n",
    "******** NOTE on RandomUndersampler: ********\n",
    "\n",
    "Across all models, the use of RandomUnderSampler consistently resulted in unusaable performance. This\n",
    "is because of significant loss of information from the majority class during undersampling, which reduced\n",
    "dataset size and limited the model's ability to learn. Initial dataset size reduction in some\n",
    "cases also contributed to this.  This was a big lesson learned.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95c3189",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "225d41a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>type</th>\n",
       "      <th>amount</th>\n",
       "      <th>nameOrig</th>\n",
       "      <th>oldbalanceOrg</th>\n",
       "      <th>newbalanceOrig</th>\n",
       "      <th>nameDest</th>\n",
       "      <th>oldbalanceDest</th>\n",
       "      <th>newbalanceDest</th>\n",
       "      <th>isFraud</th>\n",
       "      <th>isFlaggedFraud</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>9839.64</td>\n",
       "      <td>C1231006815</td>\n",
       "      <td>170136.00</td>\n",
       "      <td>160296.36</td>\n",
       "      <td>M1979787155</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>1864.28</td>\n",
       "      <td>C1666544295</td>\n",
       "      <td>21249.00</td>\n",
       "      <td>19384.72</td>\n",
       "      <td>M2044282225</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C1305486145</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C553264065</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>181.00</td>\n",
       "      <td>C840083671</td>\n",
       "      <td>181.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C38997010</td>\n",
       "      <td>21182.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>PAYMENT</td>\n",
       "      <td>11668.14</td>\n",
       "      <td>C2048537720</td>\n",
       "      <td>41554.00</td>\n",
       "      <td>29885.86</td>\n",
       "      <td>M1230701703</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362615</th>\n",
       "      <td>743</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>C786484425</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C776919290</td>\n",
       "      <td>0.00</td>\n",
       "      <td>339682.13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362616</th>\n",
       "      <td>743</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>C1529008245</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1881841831</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362617</th>\n",
       "      <td>743</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>C1162922333</td>\n",
       "      <td>6311409.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C1365125890</td>\n",
       "      <td>68488.84</td>\n",
       "      <td>6379898.11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362618</th>\n",
       "      <td>743</td>\n",
       "      <td>TRANSFER</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>C1685995037</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C2080388513</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6362619</th>\n",
       "      <td>743</td>\n",
       "      <td>CASH_OUT</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>C1280323807</td>\n",
       "      <td>850002.52</td>\n",
       "      <td>0.00</td>\n",
       "      <td>C873221189</td>\n",
       "      <td>6510099.11</td>\n",
       "      <td>7360101.63</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6362620 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         step      type      amount     nameOrig  oldbalanceOrg  \\\n",
       "0           1   PAYMENT     9839.64  C1231006815      170136.00   \n",
       "1           1   PAYMENT     1864.28  C1666544295       21249.00   \n",
       "2           1  TRANSFER      181.00  C1305486145         181.00   \n",
       "3           1  CASH_OUT      181.00   C840083671         181.00   \n",
       "4           1   PAYMENT    11668.14  C2048537720       41554.00   \n",
       "...       ...       ...         ...          ...            ...   \n",
       "6362615   743  CASH_OUT   339682.13   C786484425      339682.13   \n",
       "6362616   743  TRANSFER  6311409.28  C1529008245     6311409.28   \n",
       "6362617   743  CASH_OUT  6311409.28  C1162922333     6311409.28   \n",
       "6362618   743  TRANSFER   850002.52  C1685995037      850002.52   \n",
       "6362619   743  CASH_OUT   850002.52  C1280323807      850002.52   \n",
       "\n",
       "         newbalanceOrig     nameDest  oldbalanceDest  newbalanceDest  isFraud  \\\n",
       "0             160296.36  M1979787155            0.00            0.00        0   \n",
       "1              19384.72  M2044282225            0.00            0.00        0   \n",
       "2                  0.00   C553264065            0.00            0.00        1   \n",
       "3                  0.00    C38997010        21182.00            0.00        1   \n",
       "4              29885.86  M1230701703            0.00            0.00        0   \n",
       "...                 ...          ...             ...             ...      ...   \n",
       "6362615            0.00   C776919290            0.00       339682.13        1   \n",
       "6362616            0.00  C1881841831            0.00            0.00        1   \n",
       "6362617            0.00  C1365125890        68488.84      6379898.11        1   \n",
       "6362618            0.00  C2080388513            0.00            0.00        1   \n",
       "6362619            0.00   C873221189      6510099.11      7360101.63        1   \n",
       "\n",
       "         isFlaggedFraud  \n",
       "0                     0  \n",
       "1                     0  \n",
       "2                     0  \n",
       "3                     0  \n",
       "4                     0  \n",
       "...                 ...  \n",
       "6362615               0  \n",
       "6362616               0  \n",
       "6362617               0  \n",
       "6362618               0  \n",
       "6362619               0  \n",
       "\n",
       "[6362620 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report, roc_auc_score, make_scorer, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\ssain\\Downloads\\archive (9)\\PS_20174392719_1491204439457_log.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7c12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "########## XG BOOST ##################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b440ae5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting results for: SMOTE...\n",
      "Confusion Matrix for SMOTE:\n",
      "[[508322     31]\n",
      " [   100    557]]\n",
      "\n",
      "Getting results for: RandomUnderSampler...\n",
      "Confusion Matrix for RandomUnderSampler:\n",
      "[[508306     47]\n",
      " [    92    565]]\n",
      "\n",
      "Results for SMOTE:\n",
      "Evaluation Metrics: {'Accuracy': 0.999742637669201, 'Precision': 0.9472789115646258, 'Recall': 0.84779299847793, 'F1 Score': 0.8947791164658635, 'ROC AUC': 0.998314775880772}\n",
      "Best Hyperparameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 3, 'classifier__n_estimators': 100}\n",
      "Best CV ROC AUC Score: 0.9983\n",
      "CV Score Standard Deviation: 0.0012\n",
      "Mean Training Time (s): 4.7013\n",
      "Mean Inference Time (s): 0.1526\n",
      "\n",
      "Results for RandomUnderSampler:\n",
      "Evaluation Metrics: {'Accuracy': 0.9997269208856407, 'Precision': 0.923202614379085, 'Recall': 0.8599695585996956, 'F1 Score': 0.8904649330181245, 'ROC AUC': 0.9988582251751553}\n",
      "Best Hyperparameters: {'classifier__learning_rate': 0.1, 'classifier__max_depth': 5, 'classifier__n_estimators': 100}\n",
      "Best CV ROC AUC Score: 0.9994\n",
      "CV Score Standard Deviation: 0.0004\n",
      "Mean Training Time (s): 0.2629\n",
      "Mean Inference Time (s): 0.1760\n"
     ]
    }
   ],
   "source": [
    "#FINAL  *************\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Drop unnecessary columns that are not useful for the model\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'type', 'step'])\n",
    "\n",
    "# Create a new feature 'netAmount' to represent the net transaction amount\n",
    "df['netAmount'] = df['amount'] - df['oldbalanceOrg']\n",
    "\n",
    "# Retain only numerical columns (int and float types) for modeling\n",
    "df = df.select_dtypes(include=[int, float]).copy()\n",
    "\n",
    "# Remove rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Downsample the dataset for more manageable size, maintaining class balance\n",
    "df_sampled = df.groupby('isFraud', group_keys=False).apply(lambda x: x.sample(frac=0.40, random_state=42))\n",
    "\n",
    "# Separate features (X) and target variable (y)\n",
    "X = df_sampled.drop(['isFraud'], axis=1)\n",
    "y = df_sampled['isFraud']\n",
    "\n",
    "\n",
    "# Split data into training (60%), validation (20%), and test (20%) sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Define samplers to compare\n",
    "samplers = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42)\n",
    "}\n",
    "\n",
    "# Define hyperparameter grid for XGBoost\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [50, 100],  #number of trees\n",
    "    'classifier__max_depth': [3, 5],   #max depth\n",
    "    'classifier__learning_rate': [0.1, 0.01]\n",
    "}\n",
    "\n",
    "# dictionary to store results for each sampler\n",
    "results = {}\n",
    "\n",
    "# Iterate over each sampler (SMOTE and RandomUnderSampler)\n",
    "for sampler_name, sampler in samplers.items():\n",
    "    print(f\"\\nGetting results for: {sampler_name}...\")\n",
    "\n",
    "    # Pipeline with sampler, scaler, and classifier\n",
    "    pipeline = Pipeline([\n",
    "        ('sampler', sampler),                # Apply the current sampler (SMOTE or RandomUnderSampler)\n",
    "        ('scaler', StandardScaler()),        # Standard scaling\n",
    "        ('classifier', xgb.XGBClassifier(scale_pos_weight=len(y_train) / sum(y_train), random_state=42))\n",
    "    ])\n",
    "\n",
    "    # Perform Hyperparameter tuning using GridSearchCV with 5 fold CV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', return_train_score=True)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Get probabilities for validation set\n",
    "    proba_predictions_val = grid_search.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate precision-recall thresholds to find the optimal decision threshold\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, proba_predictions_val)\n",
    "    optimal_idx = (precision + recall).argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Evaluate model on test set using new thrshold\n",
    "    proba_predictions_test = grid_search.predict_proba(X_test)[:, 1]\n",
    "    predictions_adjusted = (proba_predictions_test >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Record evaluation metrics\n",
    "    evaluation_metrics_adjusted = {\n",
    "        'Accuracy': accuracy_score(y_test, predictions_adjusted),\n",
    "        'Precision': precision_score(y_test, predictions_adjusted, zero_division=1),\n",
    "        'Recall': recall_score(y_test, predictions_adjusted, zero_division=1),\n",
    "        'F1 Score': f1_score(y_test, predictions_adjusted, zero_division=1),\n",
    "        'ROC AUC': roc_auc_score(y_test, proba_predictions_test)\n",
    "    }\n",
    "\n",
    "    # Cross-validation results for best model\n",
    "    best_index = grid_search.best_index_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    best_model_score = cv_results['mean_test_score'][best_index]\n",
    "    best_model_score_std = cv_results['std_test_score'][best_index]\n",
    "    best_model_fit_time = cv_results['mean_fit_time'][best_index]\n",
    "    best_model_score_time = cv_results['mean_score_time'][best_index]\n",
    "\n",
    "    # Store results, including stability and efficiency metrics\n",
    "    results[sampler_name] = {\n",
    "        'Evaluation Metrics': evaluation_metrics_adjusted,\n",
    "        'Best Hyperparameters': grid_search.best_params_,\n",
    "        'Best CV ROC AUC Score': best_model_score,\n",
    "        'CV Score Std Dev': best_model_score_std,\n",
    "        'Mean Training Time (s)': best_model_fit_time,\n",
    "        'Mean Inference Time (s)': best_model_score_time\n",
    "    }\n",
    "\n",
    "# Print confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions_adjusted)\n",
    "    print(f\"Confusion Matrix for {sampler_name}:\")\n",
    "    print(cm)\n",
    "    \n",
    "# Print results\n",
    "for sampler_name, metrics in results.items():\n",
    "    print(f\"\\nResults for {sampler_name}:\")\n",
    "    print(\"Evaluation Metrics:\", metrics['Evaluation Metrics'])\n",
    "    print(\"Best Hyperparameters:\", metrics['Best Hyperparameters'])\n",
    "    print(f\"Best CV ROC AUC Score: {metrics['Best CV ROC AUC Score']:.4f}\")\n",
    "    print(f\"CV Score Standard Deviation: {metrics['CV Score Std Dev']:.4f}\")\n",
    "    print(f\"Mean Training Time (s): {metrics['Mean Training Time (s)']:.4f}\")\n",
    "    print(f\"Mean Inference Time (s): {metrics['Mean Inference Time (s)']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b00e4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e348b276",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forest ###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "959c4c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results for SMOTE:\n",
      "Best Parameters: {'classifier__max_depth': 10, 'classifier__n_estimators': 10}\n",
      "Best CV ROC AUC (SD): 0.0010177553604649844\n",
      "Mean Train Time (CV): 8.382117509841919\n",
      "Mean Inference Time (CV): 0.06881694793701172\n",
      "Evaluation Metrics on Test Data: {'Accuracy': 0.9998899826591715, 'Precision': 0.9310344827586207, 'Recall': 0.9878048780487805, 'F1 Score': 0.9585798816568047, 'ROC AUC': 0.9930865756229599}\n",
      "Confusion Matrix:\n",
      " [[190615     18]\n",
      " [     3    243]]\n",
      "\n",
      "\n",
      "Results for RandomUnderSampler:\n",
      "Best Parameters: {'classifier__max_depth': 5, 'classifier__n_estimators': 50}\n",
      "Best CV ROC AUC (SD): 0.0009538827175289722\n",
      "Mean Train Time (CV): 0.15845742225646972\n",
      "Mean Inference Time (CV): 0.20468721389770508\n",
      "Evaluation Metrics on Test Data: {'Accuracy': 0.997652963395659, 'Precision': 0.35319767441860467, 'Recall': 0.9878048780487805, 'F1 Score': 0.5203426124197003, 'ROC AUC': 0.9909105667174133}\n",
      "Confusion Matrix:\n",
      " [[190188    445]\n",
      " [     3    243]]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "# Load data\n",
    "file_path = r'C:\\Users\\ssain\\Downloads\\archive (9)\\PS_20174392719_1491204439457_log.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "\n",
    "\n",
    "# Remove irrelevant or non-predictive features from the dataset\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'type', 'step'],errors='ignore')\n",
    "\n",
    "# Create a new feature 'netAmount' to capture the net transaction amount\n",
    "# Calculated as the transaction amount minus the original balance of the sender\n",
    "df['netAmount'] = df['amount'] - df['oldbalanceOrg']\n",
    "\n",
    "# Ensure that all columns are numerical\n",
    "df = df.select_dtypes(include=[int, float]).copy()\n",
    "\n",
    "# Handle missing values by removing rows with NaN\n",
    "df = df.dropna()\n",
    "\n",
    "# Sample 15% of the dataset for faster computation while maintaining the class distribution\n",
    "df_sampled = df.groupby('isFraud', group_keys=False).apply(lambda x: x.sample(frac=0.40, random_state=42))\n",
    "\n",
    "# Split data into features and target\n",
    "X = df_sampled.drop(['isFraud'], axis=1)\n",
    "y = df_sampled['isFraud']\n",
    "\n",
    "# Train-test split with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)\n",
    "\n",
    "# Define parameter grid for RandomForest\n",
    "param_grid = {\n",
    "    'classifier__n_estimators': [10, 50],  # Number of trees in the forest\n",
    "    'classifier__max_depth': [5, 10]       # Maximum depth of each tree\n",
    "}\n",
    "\n",
    "# Define resampling methods and dictionary to store results\n",
    "resampling_methods = {'SMOTE': SMOTE(random_state=42), 'RandomUnderSampler': RandomUnderSampler(random_state=42)}\n",
    "results = {}\n",
    "\n",
    "# Loop through resampling methods\n",
    "for method_name, sampler in resampling_methods.items():\n",
    "    # Build pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('resampler', sampler),                  # Resampling method (SMOTE or RandomUnderSampler)\n",
    "        ('scaler', StandardScaler()),            # Scaling\n",
    "        ('classifier', RandomForestClassifier(class_weight='balanced', random_state=42))  # RandomForest\n",
    "    ])\n",
    "    \n",
    "     # Use 5-fold cross-validation and optimize for ROC AUC score\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='roc_auc', refit='roc_auc')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Best model cross-validation ROC AUC scores\n",
    "    best_cv_results = grid_search.cv_results_\n",
    "    best_index = grid_search.best_index_\n",
    "    best_cv_roc_auc = best_cv_results['mean_test_score'][best_index]\n",
    "    std_cv_roc_auc = best_cv_results['std_test_score'][best_index]\n",
    "    \n",
    "    # Calculate mean training and inference time from CV results\n",
    "    mean_train_time = best_cv_results['mean_fit_time'][best_index]\n",
    "    mean_inference_time = best_cv_results['mean_score_time'][best_index]\n",
    "    \n",
    "    # Predict and calculate probabilities for ROC AUC calculation\n",
    "    predictions = grid_search.predict(X_test)\n",
    "    proba_predictions = grid_search.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, predictions)\n",
    "    \n",
    "    # Evaluation metrics\n",
    "    evaluation_metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, predictions),\n",
    "        'Precision': precision_score(y_test, predictions, zero_division=1),\n",
    "        'Recall': recall_score(y_test, predictions, zero_division=1),\n",
    "        'F1 Score': f1_score(y_test, predictions, zero_division=1),\n",
    "        'ROC AUC': roc_auc_score(y_test, proba_predictions)\n",
    "    }\n",
    "    \n",
    "    # Store results\n",
    "    results[method_name] = {\n",
    "        'Best Parameters': grid_search.best_params_,\n",
    "        'Best CV ROC AUC (Mean)': best_cv_roc_auc,\n",
    "        'Best CV ROC AUC (SD)': std_cv_roc_auc,\n",
    "        'Mean Train Time (CV)': mean_train_time,\n",
    "        'Mean Inference Time (CV)': mean_inference_time,\n",
    "        'Evaluation Metrics': evaluation_metrics,\n",
    "        'Confusion Matrix':cm\n",
    "    }\n",
    "\n",
    "# Print results\n",
    "for method, result in results.items():\n",
    "    print(f\"Results for {method}:\")\n",
    "    print(\"Best Parameters:\", result['Best Parameters'])\n",
    "    print(\"Best CV ROC AUC (SD):\", result['Best CV ROC AUC (SD)'])\n",
    "    print(\"Mean Train Time (CV):\", result['Mean Train Time (CV)'])\n",
    "    print(\"Mean Inference Time (CV):\", result['Mean Inference Time (CV)'])\n",
    "    print(\"Evaluation Metrics on Test Data:\", result['Evaluation Metrics'])\n",
    "    print(\"Confusion Matrix:\\n\", result['Confusion Matrix'])\n",
    "    print(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef3eb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6875717",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP ####################################################################\n",
    "\n",
    "# This Multi-Layer Perceptron (MLP) is a feedforward neural net designed for binary classification.\n",
    "# In this implementation, the Multi-Layer Perceptron (MLP) consists of an input layer, two hidden layers with\n",
    "# ReLU activation, and an output layer with a sigmoid activation function for binary classif. The hidden\n",
    "# layers are designed with a moderate number of neurons to balance complexity and computational efficiency, while\n",
    "# dropout layers are added to prevent overfitting. This design was chosen to capture relationships\n",
    "# in the data while remaining computationally practical given resource constraints.\n",
    "\n",
    "\n",
    "# Note:create_model is a factory function used by KerasClassifier to generate new model instances dynamically.\n",
    "# Although we only define create_model once, it’s called multiple times by KerasClassifier during training,\n",
    "# cross-validation, and hyperparameter search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bc345e6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Getting results for SMOTE...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Getting results for RandomUnderSampler...\n",
      "Fitting 3 folds for each of 2 candidates, totalling 6 fits\n",
      "\n",
      "Results for SMOTE:\n",
      "Evaluation Metrics: {'Accuracy': 0.9994518771733365, 'Precision': 0.85, 'Recall': 0.6986301369863014, 'F1 Score': 0.7669172932330827, 'ROC AUC': 0.9970500714605185}\n",
      "Confusion Matrix:\n",
      " [[508272     81]\n",
      " [   198    459]]\n",
      "Best Hyperparameters: {'mlp__batch_size': 128, 'mlp__epochs': 5, 'mlp__model__dropout_rate': 0.3, 'mlp__model__neurons': 64, 'mlp__model__optimizer': 'adam'}\n",
      "Best CV ROC AUC Score: 0.9946\n",
      "CV Score Standard Deviation ROC_AUC: 0.0018\n",
      "Mean Training Time (s): 69.5803\n",
      "Mean Inference Time (s): 3.6113\n",
      "\n",
      "Results for RandomUnderSampler:\n",
      "Evaluation Metrics: {'Accuracy': 0.9989292941199583, 'Precision': 0.9745762711864406, 'Recall': 0.1750380517503805, 'F1 Score': 0.2967741935483871, 'ROC AUC': 0.9118502087984194}\n",
      "Confusion Matrix:\n",
      " [[508350      3]\n",
      " [   542    115]]\n",
      "Best Hyperparameters: {'mlp__batch_size': 128, 'mlp__epochs': 5, 'mlp__model__dropout_rate': 0.3, 'mlp__model__neurons': 128, 'mlp__model__optimizer': 'adam'}\n",
      "Best CV ROC AUC Score: 0.8812\n",
      "CV Score Standard Deviation ROC_AUC: 0.0094\n",
      "Mean Training Time (s): 1.3154\n",
      "Mean Inference Time (s): 3.4029\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,precision_recall_curve\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "import tensorflow as tf\n",
    "from scikeras.wrappers import KerasClassifier\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Load PaySim dataset\n",
    "file_path = r'C:\\Users\\ssain\\Downloads\\archive (9)\\PS_20174392719_1491204439457_log.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Data Preparation\n",
    "df = df.drop(columns=['nameOrig', 'nameDest', 'type', 'step'])\n",
    "df['netAmount'] = df['amount'] - df['oldbalanceOrg']\n",
    "df = df.select_dtypes(include=[int, float]).copy()\n",
    "df = df.dropna()\n",
    "\n",
    "# Sample the dataset to reduce computational cost\n",
    "df_sampled = df.groupby('isFraud', group_keys=False).apply(lambda x: x.sample(frac=0.40, random_state=42))\n",
    "\n",
    "# Split into features and target\n",
    "X = df_sampled.drop(['isFraud'], axis=1)\n",
    "y = df_sampled['isFraud']\n",
    "\n",
    "# Train-validation-test split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.4, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)\n",
    "\n",
    "# Define a function to create the MLP model\n",
    "def create_model(neurons=32, dropout_rate=0.3, optimizer='adam'):\n",
    "    model = tf.keras.Sequential([\n",
    "        tf.keras.layers.Input(shape=(X_train.shape[1],)),  # Input layer\n",
    "        tf.keras.layers.Dense(neurons, activation='relu'),  # First hidden layer\n",
    "        tf.keras.layers.Dropout(dropout_rate),             # Dropout for regularization\n",
    "        tf.keras.layers.Dense(neurons // 2, activation='relu'),  # Second hidden layer\n",
    "        tf.keras.layers.Dropout(dropout_rate),\n",
    "        tf.keras.layers.Dense(1, activation='sigmoid')     # Output layer for binary classification\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Wrap the MLP model using KerasClassifier\n",
    "model = KerasClassifier(model=create_model, verbose=0, random_state=42)\n",
    "\n",
    "# Define hyperparameter grid for MLP\n",
    "param_grid = {\n",
    "    'mlp__model__neurons': [64, 128], #neurons in first hidden\n",
    "    'mlp__model__dropout_rate': [0.3],\n",
    "    'mlp__epochs': [5], # No of epochs\n",
    "    'mlp__batch_size': [128],\n",
    "    'mlp__model__optimizer': ['adam'] #optimizer\n",
    "}\n",
    "\n",
    "# Define stratified K-fold CrossVal\n",
    "# Ensures class dist. preserved in every fold\n",
    "stratified_cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "# Define sampling methods for comparison\n",
    "samplers = {\n",
    "    'SMOTE': SMOTE(random_state=42),\n",
    "    'RandomUnderSampler': RandomUnderSampler(random_state=42)\n",
    "}\n",
    "\n",
    "# Results dictionary\n",
    "results = {}\n",
    "\n",
    "# Loop through each sampling method\n",
    "for sampler_name, sampler in samplers.items():\n",
    "    print(f\"\\nGetting results for {sampler_name}...\")\n",
    "\n",
    "    # Define pipeline\n",
    "    pipeline = Pipeline([\n",
    "        ('sampler', sampler),\n",
    "        ('scaler', StandardScaler()),  # Standard scaling\n",
    "        ('mlp', model)                 # MLP as the model\n",
    "    ])\n",
    "\n",
    "    # GridSearchCV for hyperparameter tuning\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=pipeline,\n",
    "        param_grid=param_grid,\n",
    "        cv=stratified_cv,  # Use 3-fold cross-validation\n",
    "        scoring='roc_auc',\n",
    "        return_train_score=True,\n",
    "        verbose=1\n",
    "    )\n",
    "    grid_search.fit(X_train, y_train)\n",
    "\n",
    "    # Predict probabilities on validation set for threshold tuning\n",
    "    proba_val = grid_search.predict_proba(X_val)[:, 1]\n",
    "    precision, recall, thresholds = precision_recall_curve(y_val, proba_val)\n",
    "    optimal_idx = (precision + recall).argmax()\n",
    "    optimal_threshold = thresholds[optimal_idx]\n",
    "\n",
    "    # Predict on test set with adjusted threshold\n",
    "    proba_test = grid_search.predict_proba(X_test)[:, 1]\n",
    "    adjusted_predictions = (proba_test >= optimal_threshold).astype(int)\n",
    "    \n",
    "    # Compute confusion matrix\n",
    "    cm = confusion_matrix(y_test, adjusted_predictions)\n",
    "    \n",
    "\n",
    "    # Evaluation metrics\n",
    "    evaluation_metrics = {\n",
    "        'Accuracy': accuracy_score(y_test, adjusted_predictions),\n",
    "        'Precision': precision_score(y_test, adjusted_predictions, zero_division=1),\n",
    "        'Recall': recall_score(y_test, adjusted_predictions, zero_division=1),\n",
    "        'F1 Score': f1_score(y_test, adjusted_predictions, zero_division=1),\n",
    "        'ROC AUC': roc_auc_score(y_test, proba_test)\n",
    "    }\n",
    "\n",
    "    # Cross-validation metrics for the best model\n",
    "    best_index = grid_search.best_index_\n",
    "    cv_results = grid_search.cv_results_\n",
    "    best_model_score = cv_results['mean_test_score'][best_index]\n",
    "    best_model_score_std = cv_results['std_test_score'][best_index]\n",
    "    best_model_fit_time = cv_results['mean_fit_time'][best_index]\n",
    "    best_model_score_time = cv_results['mean_score_time'][best_index]\n",
    "\n",
    "    # Store results\n",
    "    results[sampler_name] = {\n",
    "        'Evaluation Metrics': evaluation_metrics,\n",
    "        'Confusion Matrix':cm,\n",
    "        'Best Hyperparameters': grid_search.best_params_,\n",
    "        'Best CV ROC AUC Score': best_model_score,\n",
    "        'CV Score Std Dev': best_model_score_std,\n",
    "        'Mean Training Time (s)': best_model_fit_time,\n",
    "        'Mean Inference Time (s)': best_model_score_time\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "for sampler_name, metrics in results.items():\n",
    "    print(f\"\\nResults for {sampler_name}:\")\n",
    "    print(\"Evaluation Metrics:\", metrics['Evaluation Metrics'])\n",
    "    print(\"Confusion Matrix:\\n\", metrics['Confusion Matrix'])\n",
    "    print(\"Best Hyperparameters:\", metrics['Best Hyperparameters'])\n",
    "    print(f\"Best CV ROC AUC Score: {metrics['Best CV ROC AUC Score']:.4f}\")\n",
    "    print(f\"CV Score Standard Deviation ROC_AUC: {metrics['CV Score Std Dev']:.4f}\")\n",
    "    print(f\"Mean Training Time (s): {metrics['Mean Training Time (s)']:.4f}\")\n",
    "    print(f\"Mean Inference Time (s): {metrics['Mean Inference Time (s)']:.4f}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
